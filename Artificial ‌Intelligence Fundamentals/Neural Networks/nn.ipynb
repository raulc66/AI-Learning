{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVB7klrtGHHX"
   },
   "source": [
    "#Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iStct0cYPCe7"
   },
   "source": [
    "In this part of the laboratory, you are to complete the API for a Neural Net. Then afterwards, you are to construct various neural nets using the API to solve abstract learning problems.\n",
    "\n",
    "## Completing the implementation\n",
    "\n",
    "We have provided you a skeleton of the Neural Net in the Neural net code. You are to complete the unimplemented methods.\n",
    "\n",
    "The three classes `Input`, `PerformanceElem`, and `Neuron` all have incomplete implementation of the following function:\n",
    "     \n",
    "    def output(self)\n",
    "\n",
    "Your first task is to fill in all 3 functions to complete the API.\n",
    "\n",
    "## Output\n",
    "\n",
    "The function `output(self)` produces the output of each of these elements.\n",
    "\n",
    "Be sure to use the sigmoid activation function:\n",
    "\n",
    "    o = s(z) = 1.0 / (1.0 + e**(-z))\n",
    "\n",
    "and the performance function and its derivative as discussed in the course:\n",
    "\n",
    "    P(o) = -0.5 (d - o)**2\n",
    "    \n",
    "## Derivatives\n",
    "\n",
    "The function `dOutdX(self, elem)` generates the value of the partial derivative with respect to a given weight element.\n",
    "\n",
    "Recall, neural nets update a given weight by computing the partial derivative of the performance function with respect to that weight. The formula we have used in class is as follows:\n",
    "\n",
    "    wi' = wi + rate * dP / dwi\n",
    "    \n",
    "In our code this is represented as (see `def train()` -- you don't have to implement this):\n",
    "\n",
    "    w.set_next_value( w.get_value() + rate * network.performance.dOutdX(w) )\n",
    "The element passed to the `dOutdX` function is always a weight. Namely it is the weight that we are doing the partial over. Your job is to figure out how to define `dOutdX()` in terms of recursively calling `dOutdX()` or `output()` over the inputs and weights of a network element.\n",
    "\n",
    "For example, consider the Performance element P, `P.dOutdX(w)` could be defined in the following recursive fashion:\n",
    "\n",
    "    dP/d(w) = dP/do * do/dw # applying the chain rule\n",
    "            = (d-o) * o.dOutdX(w)\n",
    "Here `o` is the output of the Neuron that is directly connected to P.\n",
    "\n",
    "For Neuron units, there would be more cases to consider. Namely,\n",
    "\n",
    "\n",
    "1.   The termination case where the weight being differentiated over is one of the (direct) weights of the current neuron.\n",
    "2.   The recursive case where the weight is not one that is directly connected (but is a descendant weight).\n",
    "\n",
    "This implementation models the process of computing the chain of derivatives recursively. This top-down approach works from the output layer towards the input layers. This is in contrast to the more conventional approach (taught in the course) where you computed a per-node little-delta, and then recursively computed the weight updates bottom-up, from input towards output layers.\n",
    "\n",
    "## About the API Classes\n",
    "\n",
    "Most of the classes in the Neural Network inherit from the following two abstract classes:\n",
    "\n",
    "### `ValuedElement`\n",
    "\n",
    "This interface class allows an element to have a settable value. `Input` (e.g. i1, i2) and `Weight` (e.g. w1A, wAB) are subclasses of `ValueElement`.\n",
    "\n",
    "Elements that are subclassed all have these methods:\n",
    "\n",
    "*  `set_value(self,val)` - set the value of the element\n",
    "*  `get_value(self)` - get the value of the element\n",
    "*  `get_name(self)` - get the name of the element\n",
    "\n",
    "### `DifferentiableElement`\n",
    "\n",
    "This abstract class defines the interface for elements that have outputs and are involved in partial derivatives.\n",
    "\n",
    "* `output(self):` returns the output of this element\n",
    "* `dOutdX(self,elem):` returns the partial derivative with respect to another element\n",
    "\n",
    "`Inputs`, `Neurons`, and `PerformanceElem` are the three subclasses that implement `DifferentiableElement`. You will have to complete the interface for these classes.\n",
    "\n",
    "### `Weight(ValuedElement)`\n",
    "\n",
    "Represents update-able weights in the network. In addition to `ValueElement` functions are the following methods, which are used for the training algorithm (you will not need them in your implementation):\n",
    "\n",
    "* `set_next_value(self,val):` which sets the next weight value in `self.next_value`\n",
    "* `update(self):` which sets the current weight to the value stored in `self.next_value`\n",
    "\n",
    "### `Input(DifferentiableElement, ValuedElement)`\n",
    "\n",
    "Represents inputs to the network. These may represent variable inputs as well as fixed inputs (i.e. threshold inputs) that are always set to -1. `output()` of `Input` units should simply return the value they are set to during training or testing.\n",
    "\n",
    "### `Neuron(DifferentiableElement)`\n",
    "\n",
    "Represents the actual neurons in the neural net. The definition for `output` already contains some code. Namely, we've implemented a value caching mechanism to speed up the training / testing process. So instead of implementing output directly you should instead implement:\n",
    "\n",
    "* `compute_output(self):`\n",
    "\n",
    "### `PerformanceElem(DifferentiableElement)`\n",
    "\n",
    "Represents a Performance Element that allows you to set the desired output.\n",
    "\n",
    "* `set_desired` which sets `my_desired_val`\n",
    "\n",
    "To better understand back-propagation, you should take a look at the methods `train` and `test` in the Neural net code to see how everything is put together.\n",
    "\n",
    "## Unit Testing\n",
    "\n",
    "Once you've completed the missing functions, we have provided the Neural net tester code to help you unit test. You will need to pass the tests in this unit tester before you can move on to the next parts.\n",
    "\n",
    "To check if your implementation works, run:\n",
    "\n",
    "    neural_net_tester([\"simple\"])\n",
    "This makes sure that your code works and can learn basic functions such as AND and OR.\n",
    "\n",
    "## Building Neural Nets\n",
    "\n",
    "Once you have finished implementing the Neural Net API, you will be tasked to build three networks to learn various abstract data sets.\n",
    "\n",
    "Here is an example of how to construct a basic neural network:\n",
    "\n",
    "    def make_neural_net_basic():\n",
    "       \"\"\"Returns a 1 neuron network with 2 variable inputs, and 1 fixed input.\"\"\"\n",
    "       i0 = Input('i0',-1.0) # this input is immutable\n",
    "       i1 = Input('i1',0.0)\n",
    "       i2 = Input('i2',0.0)\n",
    "       \n",
    "       w1A = Weight('w1A',1)\n",
    "       w2A = Weight('w2A',1)\n",
    "       wA = Weight('wA', 1)\n",
    "       \n",
    "       # the inputs must be in the same order as their associated weights\n",
    "       A = Neuron('A', [i1,i2,i0], [w1A,w2A,wA])\n",
    "       P = PerformanceElem(A, 0.0)\n",
    "       \n",
    "       # Package all the components into a network\n",
    "       # First list the PerformanceElem P\n",
    "       # Then list all neurons afterwards\n",
    "       net = Network(P,[A])\n",
    "       \n",
    "       return net\n",
    "       \n",
    "## Naming conventions\n",
    "\n",
    "IMPORTANT: Be sure to use the following naming convention when creating elements for your networks:\n",
    "\n",
    "Inputs:\n",
    "\n",
    "* Format: `'i'` + input_number\n",
    "* Conventions:\n",
    "   * Start numbering from 1.\n",
    "   * Use the same `i0` for all the fixed -1 inputs.\n",
    "* Examples: `'i1'`, `i2`.\n",
    "\n",
    "Weights:\n",
    "* Format `'w' + from_identifier + to_identifier`\n",
    "* Examples:\n",
    "   * `w1A` for weight from Input `i1` to Neuron `A`.\n",
    "   * `wBC` for weight from Neuron `B` to Neuron `C`.\n",
    "   \n",
    "Neurons:\n",
    "\n",
    "* Format: `alphabet_letter`.\n",
    "* Convention: Assign neuron names in order of distance to the inputs.\n",
    "* Example: `A` is the neuron closest to the inputs, and on the left-most (or top-most) side of the net.\n",
    "* For ties, order neurons from left to right or top to bottom (depending on how you draw orient your network).\n",
    "\n",
    "## Building a 2-layer Neural Net\n",
    "\n",
    "Now use the Neural Net API you've just completed and tested to create a two layer network that looks like the following. \n",
    "\n",
    "![alt text](https://ai6034.mit.edu/wiki/images/NeuralNet.png)\n",
    "\n",
    "Fill your answer in the function stub:\n",
    "   \n",
    "    def make_neural_net_two_layer()\n",
    "in the Neural net code. \n",
    "\n",
    "Your 2-layer neural net should now be able to learn slightly harder datasets, such as the classic non-linearly separable examples such as NOT-EQUAL (XOR) and EQUAL.\n",
    "\n",
    "When initializing the weights of the network, you should use random weights. To get deterministic random initial weights so that tests are reproducible you should first seed the random number generator, and then generate the random weights.\n",
    "\n",
    "    seed_random()\n",
    "    \n",
    "    wt = random_weight()\n",
    "    ...use wt...\n",
    "    wt2 = random_weight()\n",
    "    ...use wt2...\n",
    "    \n",
    "Note: the function `random_weight()` in the Neural net code uses the python function `random.randrange(-1,2)` to compute initial weights. This function generates values: -1, 0, 1 (randomly). While this may seem like a mistake, what we've found empirically is that this actually performs better than using `random.uniform(-1, 1)`. Be our guest and play around with the `random_weight` function. You'll find that Neural Nets can be quite sensitive to initialization weight settings. (Recall what happens if you set all weights to the same value.)    \n",
    "\n",
    "To test your completed network, run:\n",
    "\n",
    "    neural_net_tester([\"two_layer\"])\n",
    "Your network should learn and classify all the datasets in the `harder_data_set` from the Neural net data code with 100% accuracy.\n",
    "\n",
    "## Designing For More Challenging Datasets\n",
    "\n",
    "Now it's your turn to design the network. We want to be able to classify more complex data sets.\n",
    "\n",
    "Specifically we want you to design a new network that should theoretically be able to learn and classify the following datasets:\n",
    "\n",
    "1. The letter-L.\n",
    "\n",
    "       4 + -\n",
    "       3 + -\n",
    "       2 + -\n",
    "       1 + - - - -\n",
    "       0 - + + + +\n",
    "         0 1 2 3 4\n",
    "2. This moat-like shape:\n",
    "\n",
    "       4 - - - - -\n",
    "       3 -       -\n",
    "       2 -   +   -\n",
    "       1 -       -\n",
    "       0 - - - - -\n",
    "         0 1 2 3 4\n",
    "3. This `patchy` shape:\n",
    "\n",
    "       4 - -   + +\n",
    "       3 - -   + +\n",
    "       2\n",
    "       1 + +   - -\n",
    "       0 + +   - -\n",
    "         0 1 2 3 4\n",
    "         \n",
    "We claim that a network architecture containing 5 neuron nodes or less can fully learn and classify all three shapes. In fact, we require it!\n",
    "\n",
    "Construct a new network in:\n",
    "\n",
    "    def make_neural_net_challenging()\n",
    "that can (theoretically) perfectly learn and classify all three datasets.\n",
    "\n",
    "To test your network on the first 2 of these shapes, run\n",
    "\n",
    "    neural_net_tester([\"challenging\"])\n",
    "To pass our tests, your network must get 100% accuracy within 10000 iterations.\n",
    "\n",
    "Now try your architecture on the third dataset, patchy. Run:\n",
    "\n",
    "    neural_net_tester([\"patchy\"])\n",
    "Depending on your architecture and your initial weights, your network may either easily learn patchy or get stuck in a local maximum. Does your network completely learn the dataset within 10000 iterations? If not, take a look at the weights output at the end of the 10000 iterations. Plot the weights in terms of a linear function on a 2D graph. Do the boundaries tell you why there might be a local maximum?\n",
    "    \n",
    "If everything tests with an accuracy of 1.0, then you've completed the Neural Networks laboratory.\n",
    "\n",
    "# Hints\n",
    "\n",
    "If you are having problems with getting your network to convergence on certain problems, try the following:\n",
    "1. Order your weight initialization (i.e. calls to `random_weight()`) from the bottom-most weights to the top-most weights in you network. While this ordering theoretically irrelevant, we've found that this ordering worked well in practice (in conjunction with 1 above). NN are unfortunately quite sensitive to initial weight settings.\n",
    "2. Play around with the `seed_random` function to try different starting random seeds, although seeding the random function with 0 is what worked for us.\n",
    "3. If none of these work, try setting weights that are close to what you want in terms of the final expected solution.\n",
    "\n",
    "**Q:** How can I tell which input goes with which weight in the `my_inputs` and `my_weights` lists in a Neuron?\n",
    "\n",
    "**A:** The two lists are in the same order - `my_weights[0]` is the weight for `my_inputs[0]`, etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMJx0c2jnkI0"
   },
   "source": [
    "# Neural nets - To be implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HLdORYx4ntE3"
   },
   "outputs": [],
   "source": [
    "# - In this file we have an incomplete skeleton of\n",
    "# a neural network implementation.  Follow the instructions\n",
    "# and complete the NotImplemented methods below.\n",
    "#\n",
    "\n",
    "import math\n",
    "import random\n",
    "from functools import cmp_to_key\n",
    "\n",
    "class ValuedElement(object):\n",
    "    \"\"\"\n",
    "    This is an abstract class that all Network elements inherit from\n",
    "    \"\"\"\n",
    "    def __init__(self,name,val):\n",
    "        self.my_name = name\n",
    "        self.my_value = val\n",
    "\n",
    "    def set_value(self,val):\n",
    "        self.my_value = val\n",
    "\n",
    "    def get_value(self):\n",
    "        return self.my_value\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.my_name\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"%s(%s)\" %(self.my_name, self.my_value)\n",
    "\n",
    "class DifferentiableElement(object):\n",
    "    \"\"\"\n",
    "    This is an abstract interface class implemented by all Network\n",
    "    parts that require some differentiable element.\n",
    "    \"\"\"\n",
    "    def output(self):\n",
    "        weighted_sum = sum(w.get_value() * w.get_input().output() for w in self.weights)\n",
    "        return self.activation_function(weighted_sum)\n",
    "\n",
    "    def dOutdX(self, elem):\n",
    "        if elem in self.weights:\n",
    "            return self.get_value() * (1 - self.get_value()) * elem.get_input().output()\n",
    "        \n",
    "        return sum(w.get_value() * w.get_input().output() * (1 - self.output()) * self.output()\n",
    "                   * elem.dOutdX(w) for w in self.weights if elem in w.get_input().get_ancestors())\n",
    "\n",
    "    def clear_cache(self):\n",
    "        \"\"\"clears any precalculated cached value\"\"\"\n",
    "        pass\n",
    "\n",
    "class Input(ValuedElement,DifferentiableElement):\n",
    "    \"\"\"\n",
    "    Representation of an Input into the network.\n",
    "    These may represent variable inputs as well as fixed inputs\n",
    "    (Thresholds) that are always set to -1.\n",
    "    \"\"\"\n",
    "    def __init__(self,name,val):\n",
    "        ValuedElement.__init__(self,name,val)\n",
    "        DifferentiableElement.__init__(self)\n",
    "\n",
    "    def output(self):\n",
    "        \"\"\"\n",
    "        Returns the output of this Input node.\n",
    "        \n",
    "        returns: number (float or int)\n",
    "        \"\"\"\n",
    "        return self.get_value()\n",
    "\n",
    "    def dOutdX(self, elem):\n",
    "        \"\"\"\n",
    "        Returns the derivative of this Input node with respect to \n",
    "        elem.\n",
    "\n",
    "        elem: an instance of Weight\n",
    "\n",
    "        returns: number (float or int)\n",
    "        \"\"\"\n",
    "        return 0\n",
    "\n",
    "class Weight(ValuedElement):\n",
    "    \"\"\"\n",
    "    Representation of an weight into a Neural Unit.\n",
    "    \"\"\"\n",
    "    def __init__(self,name,val):\n",
    "        ValuedElement.__init__(self,name,val)\n",
    "        self.next_value = None\n",
    "\n",
    "    def set_next_value(self,val):\n",
    "        self.next_value = val\n",
    "\n",
    "    def update(self):\n",
    "        self.my_value = self.next_value\n",
    "\n",
    "class Neuron(DifferentiableElement):\n",
    "    \"\"\"\n",
    "    Representation of a single sigmoid Neural Unit.\n",
    "    \"\"\"\n",
    "    def __init__(self, name, inputs, input_weights, use_cache=True):\n",
    "        assert len(inputs)==len(input_weights)\n",
    "        for i in range(len(inputs)):\n",
    "            assert isinstance(inputs[i],(Neuron,Input))\n",
    "            assert isinstance(input_weights[i],Weight)\n",
    "        DifferentiableElement.__init__(self)\n",
    "        self.my_name = name\n",
    "        self.my_inputs = inputs # list of Neuron or Input instances\n",
    "        self.my_weights = input_weights # list of Weight instances\n",
    "        self.use_cache = use_cache\n",
    "        self.clear_cache()\n",
    "        self.my_descendant_weights = None\n",
    "\n",
    "    def get_descendant_weights(self):\n",
    "        \"\"\"\n",
    "        Returns a mapping of the names of direct weights into this neuron,\n",
    "        to all descendant weights.\n",
    "        \"\"\"\n",
    "        if self.my_descendant_weights is None:\n",
    "            self.my_descendant_weights = {}\n",
    "            inputs = self.get_inputs()\n",
    "            weights = self.get_weights()\n",
    "            for i in range(len(weights)):\n",
    "                weight = weights[i]\n",
    "                weight_name = weight.get_name()\n",
    "                self.my_descendant_weights[weight_name] = set()\n",
    "                input = inputs[i]\n",
    "                if not isinstance(input, Input):\n",
    "                    descendants = input.get_descendant_weights()\n",
    "                    for name, s in descendants.items():\n",
    "                        st = self.my_descendant_weights[weight_name]\n",
    "                        st = st.union(s)\n",
    "                        st.add(name)\n",
    "                        self.my_descendant_weights[weight_name] = st\n",
    "\n",
    "        return self.my_descendant_weights\n",
    "\n",
    "    def isa_descendant_weight_of(self, target, weight):\n",
    "        \"\"\"\n",
    "        Checks if [target] is a indirect input weight into this Neuron\n",
    "        via the direct input weight [weight].\n",
    "        \"\"\"\n",
    "        weights = self.get_descendant_weights()\n",
    "        if weight.get_name() in weights:\n",
    "            return target.get_name() in weights[weight.get_name()]\n",
    "        else:\n",
    "            raise Exception(\"weight %s is not connect to this node: %s\"\n",
    "                            %(weight, self))\n",
    "\n",
    "    def has_weight(self, weight):\n",
    "        \"\"\"\n",
    "        Checks if [weight] is a direct input weight into this Neuron.\n",
    "        \"\"\"\n",
    "        weights = self.get_descendant_weights()\n",
    "        return weight.get_name() in self.get_descendant_weights()\n",
    "\n",
    "    def get_weight_nodes(self):\n",
    "        return self.my_weights\n",
    "\n",
    "    def clear_cache(self):\n",
    "        self.my_output = None\n",
    "        self.my_doutdx = {}\n",
    "\n",
    "    def output(self):\n",
    "        # Implement compute_output instead!!\n",
    "        if self.use_cache:\n",
    "            # caching optimization, saves previously computed dOutDx.\n",
    "            if self.my_output is None:\n",
    "                self.my_output = self.compute_output()\n",
    "            return self.my_output\n",
    "        return self.compute_output()\n",
    "\n",
    "    def compute_output(self):\n",
    "        \"\"\"\n",
    "        Returns the output of this Neuron node, using a sigmoid as\n",
    "        the threshold function.\n",
    "\n",
    "        returns: number (float or int)\n",
    "        \"\"\"\n",
    "        weighted_sum = sum(w.get_value() * i.output() for w, i in zip(self.my_weights, self.my_inputs))\n",
    "        return 1.0 / (1.0 + math.exp(-weighted_sum))\n",
    "\n",
    "    def dOutdX(self, elem):\n",
    "        # Implement compute_doutdx instead!!\n",
    "        if self.use_cache:\n",
    "            # caching optimization, saves previously computed dOutDx.\n",
    "            if elem not in self.my_doutdx:\n",
    "                self.my_doutdx[elem] = self.compute_doutdx(elem)\n",
    "            return self.my_doutdx[elem]\n",
    "        return self.compute_doutdx(elem)\n",
    "\n",
    "    def compute_doutdx(self, elem):\n",
    "        \"\"\"\n",
    "        Returns the derivative of this Neuron node, with respect to weight\n",
    "        elem, calling output() and/or dOutdX() recursively over the inputs.\n",
    "\n",
    "        elem: an instance of Weight\n",
    "\n",
    "        returns: number (float/int)\n",
    "        \"\"\"\n",
    "        \n",
    "        out = self.output()\n",
    "        octerm = out * (1 - out)\n",
    "\n",
    "        if self.has_weight(elem):\n",
    "            index = self.my_weights.index(elem)\n",
    "            oa = self.get_inputs()[index].output()\n",
    "            d = octerm*oa\n",
    "        else:\n",
    "            d = 0\n",
    "            for i in range(len(self.get_weights())):\n",
    "                cur_w = self.my_weights[i]\n",
    "                if self.isa_descendant_weight_of(elem, cur_w):\n",
    "                    input_deriv = self.get_inputs()[i].dOutdX(elem)\n",
    "                    d += cur_w.get_value() * input_deriv\n",
    "            d *= octerm\n",
    "        return d\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.my_weights\n",
    "\n",
    "    def get_inputs(self):\n",
    "        return self.my_inputs\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.my_name\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Neuron(%s)\" %(self.my_name)\n",
    "\n",
    "\n",
    "class PerformanceElem(DifferentiableElement):\n",
    "    \"\"\"\n",
    "    Representation of a performance computing output node.\n",
    "    This element contains methods for setting the\n",
    "    desired output (d) and also computing the final\n",
    "    performance P of the network.\n",
    "\n",
    "    This implementation assumes a single output.\n",
    "    \"\"\"\n",
    "    def __init__(self,input,desired_value):\n",
    "        assert isinstance(input,(Input,Neuron))\n",
    "        DifferentiableElement.__init__(self)\n",
    "        self.my_input = input\n",
    "        self.my_desired_val = desired_value\n",
    "\n",
    "    def output(self):\n",
    "        \"\"\"\n",
    "        Returns the output of this PerformanceElem node.\n",
    "        \n",
    "        returns: number (float/int)\n",
    "        \"\"\"\n",
    "        return -0.5 * (self.my_desired_val - self.my_input.output())**2\n",
    "\n",
    "    def dOutdX(self, elem):\n",
    "        \"\"\"\n",
    "        Returns the derivative of this PerformanceElem node with respect\n",
    "        to some weight, given by elem.\n",
    "\n",
    "        elem: an instance of Weight\n",
    "        returns: number (int/float)\n",
    "        \"\"\"\n",
    "        return (self.my_desired_val - self.my_input.output()) * self.my_input.dOutdX(elem)\n",
    "\n",
    "    def set_desired(self,new_desired):\n",
    "        self.my_desired_val = new_desired\n",
    "\n",
    "    def get_input(self):\n",
    "        return self.my_input\n",
    "\n",
    "def alphabetize(x,y):\n",
    "    if x.get_name()>y.get_name():\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self,performance_node,neurons):\n",
    "        self.inputs =  []\n",
    "        self.weights = []\n",
    "        self.performance = performance_node\n",
    "        self.output = performance_node.get_input()\n",
    "        self.neurons = neurons[:]\n",
    "        self.neurons.sort(key=cmp_to_key(alphabetize))\n",
    "        for neuron in self.neurons:\n",
    "            self.weights.extend(neuron.get_weights())\n",
    "            for i in neuron.get_inputs():\n",
    "                if isinstance(i,Input) and not i.get_name()=='i0' and not i in self.inputs:\n",
    "                    self.inputs.append(i)\n",
    "        self.weights.reverse()\n",
    "        self.weights = []\n",
    "        for n in self.neurons:\n",
    "            self.weights += n.get_weight_nodes()\n",
    "\n",
    "    def clear_cache(self):\n",
    "        for n in self.neurons:\n",
    "            n.clear_cache()\n",
    "\n",
    "def seed_random():\n",
    "    \"\"\"Seed the random number generator so that random\n",
    "    numbers are deterministically 'random'\"\"\"\n",
    "    random.seed(0)\n",
    "\n",
    "def random_weight():\n",
    "    \"\"\"Generate a deterministic random weight\"\"\"\n",
    "    # We found that random.randrange(-1,2) to work well emperically \n",
    "    # even though it produces randomly 3 integer values -1, 0, and 1.\n",
    "    return random.randrange(-1, 2)\n",
    "\n",
    "    # Uncomment the following if you want to try a uniform distribuiton \n",
    "    # of random numbers compare and see what the difference is.\n",
    "    # return random.uniform(-1, 1)\n",
    "\n",
    "def make_neural_net_basic():\n",
    "    \"\"\"\n",
    "    Constructs a 2-input, 1-output Network with a single neuron.\n",
    "    This network is used to test your network implementation\n",
    "    and a guide for constructing more complex networks.\n",
    "\n",
    "    Naming convention for each of the elements:\n",
    "\n",
    "    Input: 'i'+ input_number\n",
    "    Example: 'i1', 'i2', etc.\n",
    "    Conventions: Start numbering at 1.\n",
    "                 For the -1 inputs, use 'i0' for everything\n",
    "\n",
    "    Weight: 'w' + from_identifier + to_identifier\n",
    "    Examples: 'w1A' for weight from Input i1 to Neuron A\n",
    "              'wAB' for weight from Neuron A to Neuron B\n",
    "\n",
    "    Neuron: alphabet_letter\n",
    "    Convention: Order names by distance to the inputs.\n",
    "                If equal distant, then order them left to right.\n",
    "    Example:  'A' is the neuron closest to the inputs.\n",
    "\n",
    "    All names should be unique.\n",
    "    You must follow these conventions in order to pass all the tests.\n",
    "    \"\"\"\n",
    "    i0 = Input('i0', -1.0) # this input is immutable\n",
    "    i1 = Input('i1', 0.0)\n",
    "    i2 = Input('i2', 0.0)\n",
    "\n",
    "    w1A = Weight('w1A', 1)\n",
    "    w2A = Weight('w2A', 1)\n",
    "    wA  = Weight('wA', 1)\n",
    "\n",
    "    # Inputs must be in the same order as their associated weights\n",
    "    A = Neuron('A', [i1,i2,i0], [w1A,w2A,wA])\n",
    "    P = PerformanceElem(A, 0.0)\n",
    "\n",
    "    net = Network(P,[A])\n",
    "    return net\n",
    "\n",
    "def make_neural_net_two_layer():\n",
    "    \"\"\"\n",
    "    Create a 2-input, 1-output Network with three neurons.\n",
    "    There should be two neurons at the first level, each receiving both inputs\n",
    "    Both of the first level neurons should feed into the second layer neuron.\n",
    "\n",
    "    See 'make_neural_net_basic' for required naming convention for inputs,\n",
    "    weights, and neurons.\n",
    "    \"\"\"\n",
    "    \n",
    "    i0 = Input('i0', -1.0)\n",
    "    i1 = Input('i1', 0.0)\n",
    "    i2 = Input('i2', 0.0)\n",
    "    \n",
    "    w1A = Weight('w1A', random_weight())\n",
    "    w2A = Weight('w2A', random_weight())\n",
    "    wA = Weight('wA', random_weight())\n",
    "    \n",
    "    w1B = Weight('w1B', random_weight())\n",
    "    w2B = Weight('w2B', random_weight())\n",
    "    wB = Weight('wB', random_weight())\n",
    "    \n",
    "    A = Neuron('A', [i1, i2, i0], [w1A, w2A, wA])\n",
    "    B = Neuron('B', [i1, i2, i0],[w1B, w2B, wB])\n",
    "    \n",
    "    wAC = Weight('AB', random_weight())\n",
    "    wBC = Weight('BC', random_weight())\n",
    "    wC = Weight('wC', random_weight())\n",
    "    \n",
    "    C = Neuron('C', [A, B, i0], [wAC, wBC,wC])\n",
    "    P = PerformanceElem(C, 0.0)\n",
    "    \n",
    "    net = Network(P, [A, B, C])\n",
    "    return net\n",
    "\n",
    "def make_neural_net_challenging():\n",
    "    \"\"\"\n",
    "    Design a network that can in-theory solve all 3 problems described in\n",
    "    the lab instructions.  Your final network should contain\n",
    "    at most 5 neuron units.\n",
    "\n",
    "    See 'make_neural_net_basic' for required naming convention for inputs,\n",
    "    weights, and neurons.\n",
    "    \"\"\"\n",
    "    i0 = Input('i0', -1.0)\n",
    "    i1 = Input('i1', 0.0)\n",
    "    i2 = Input('i2', 0.0)\n",
    "\n",
    "    w1A = Weight('w1A', random_weight())\n",
    "    w2A = Weight('w2A', random_weight())\n",
    "    wA = Weight('wA', random_weight())\n",
    "\n",
    "    w1B = Weight('w1B', random_weight())\n",
    "    w2B = Weight('w2B', random_weight())\n",
    "    wB = Weight('wB', random_weight())\n",
    "\n",
    "    wAC = Weight('wAC', random_weight())\n",
    "    wBC = Weight('wBC', random_weight())\n",
    "    wC = Weight('wC', random_weight())\n",
    "    \n",
    "    wABCD = Weight('wABCD', random_weight())\n",
    "    #wBCD = Weight('wBCD', random_weight())\n",
    "    wD = Weight('wD',random_weight())\n",
    "\n",
    "    A = Neuron('A', [i1, i2, i0], [w1A, w2A, wA])\n",
    "    B = Neuron('B', [i1, i2, i0], [w1B, w2B, wB])\n",
    "    C = Neuron('C', [A, B, i0], [wAC, wBC, wC])\n",
    "\n",
    "    D = Neuron('D', [C, i0], [wABCD, wD])\n",
    "    \n",
    "    P = PerformanceElem(D, 0.0)\n",
    "    net = Network(P, [A, B ,C ,D])\n",
    "    \n",
    "    return net\n",
    "\n",
    "def make_net_with_init_weights_from_dict(net_fn,init_weights):\n",
    "    net = net_fn()\n",
    "    for w in net.weights:\n",
    "        w.set_value(init_weights[w.get_name()])\n",
    "    return net\n",
    "\n",
    "def make_net_with_init_weights_from_list(net_fn,init_weights):\n",
    "    net = net_fn()\n",
    "    for i in range(len(net.weights)):\n",
    "        net.weights[i].set_value(init_weights[i])\n",
    "    return net\n",
    "\n",
    "\n",
    "def abs_mean(values):\n",
    "    \"\"\"Compute the mean of the absolute values a set of numbers.\n",
    "    For computing the stopping condition for training neural nets\"\"\"\n",
    "    abs_vals = map(lambda x: abs(x), values)\n",
    "    my_list = []\n",
    "    for a in values:\n",
    "        my_list.append(abs(a))\n",
    "    total = sum(my_list)\n",
    "    aux = len(my_list)\n",
    "    return total / aux\n",
    "\n",
    "\n",
    "def train(network,\n",
    "          data,      # training data\n",
    "          rate=1.0,  # learning rate\n",
    "          target_abs_mean_performance=0.0001,\n",
    "          max_iterations=10000,\n",
    "          verbose=False):\n",
    "    \"\"\"Run back-propagation training algorithm on a given network.\n",
    "    with training [data].   The training runs for [max_iterations]\n",
    "    or until [target_abs_mean_performance] is reached.\n",
    "    \"\"\"\n",
    "    iteration = 0\n",
    "    while iteration < max_iterations:\n",
    "        fully_trained = False\n",
    "        performances = []  # store performance on each data point\n",
    "        for datum in data:\n",
    "            # set network inputs\n",
    "            for i in range(len(network.inputs)):\n",
    "                network.inputs[i].set_value(datum[i])\n",
    "\n",
    "            # set network desired output\n",
    "            network.performance.set_desired(datum[-1])\n",
    "\n",
    "            # clear cached calculations\n",
    "            network.clear_cache()\n",
    "\n",
    "            # compute all the weight updates\n",
    "            for w in network.weights:\n",
    "                w.set_next_value(w.get_value() +\n",
    "                                 rate * network.performance.dOutdX(w))\n",
    "\n",
    "            # set the new weights\n",
    "            for w in network.weights:\n",
    "                w.update()\n",
    "\n",
    "            # save the performance value\n",
    "            performances.append(network.performance.output())\n",
    "\n",
    "            # clear cached calculations\n",
    "            network.clear_cache()\n",
    "\n",
    "        # compute the mean performance value\n",
    "        abs_mean_performance = abs_mean(performances)\n",
    "\n",
    "        if abs_mean_performance < target_abs_mean_performance:\n",
    "            if verbose:\n",
    "                print(\"iter %d: training complete.\\n\"\\\n",
    "                      \"mean-abs-performance threshold %s reached (%1.6f)\"\\\n",
    "                      %(iteration,\n",
    "                        target_abs_mean_performance,\n",
    "                        abs_mean_performance))\n",
    "            break\n",
    "\n",
    "        iteration += 1\n",
    "        if iteration % 1000 == 0 and verbose:\n",
    "            print(\"iter %d: mean-abs-performance = %1.6f\"\\\n",
    "                  %(iteration,\n",
    "                    abs_mean_performance))\n",
    "\n",
    "\n",
    "def test(network, data, verbose=False):\n",
    "    \"\"\"Test the neural net on some given data.\"\"\"\n",
    "    correct = 0\n",
    "    for datum in data:\n",
    "\n",
    "        for i in range(len(network.inputs)):\n",
    "            network.inputs[i].set_value(datum[i])\n",
    "\n",
    "        # clear cached calculations\n",
    "        network.clear_cache()\n",
    "        result = network.output.output()\n",
    "        network.clear_cache()\n",
    "\n",
    "        rounded_result = round(result)\n",
    "        if round(result)==datum[-1]:\n",
    "            correct+=1\n",
    "            if verbose:\n",
    "                print(\"test(%s) returned: %s => %s [%s]\" %(str(datum),\n",
    "                                                           str(result),\n",
    "                                                           rounded_result,\n",
    "                                                           \"correct\"))\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"test(%s) returned: %s => %s [%s]\" %(str(datum),\n",
    "                                                           str(result),\n",
    "                                                           rounded_result,\n",
    "                                                           \"wrong\"))\n",
    "\n",
    "    return float(correct)/len(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "569hIE5WoDHf"
   },
   "source": [
    "# Neural net data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oNa_tZP-oFON"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Training and Test Data used in the Neural net tester code.\n",
    "#\n",
    "\"\"\"\n",
    "1++\n",
    "0-+\n",
    " 01\n",
    "\"\"\"\n",
    "or_data = ((0,0,0),\n",
    "           (0,1,1),\n",
    "           (1,0,1),\n",
    "           (1,1,1),\n",
    "           (0.25,0,0),\n",
    "           (0,0.25,0))\n",
    "\n",
    "or_test_data = ((0.1,0.1,0),\n",
    "                (0.1,0.9,1),\n",
    "                (0.9,0.1,1),\n",
    "                (0.9,0.9,1))\n",
    "\"\"\"\n",
    "1-+\n",
    "0--\n",
    " 01\n",
    "\"\"\"\n",
    "and_data = ((0,0,0),\n",
    "            (0,1,0),\n",
    "            (1,0,0),\n",
    "            (1,1,1),\n",
    "            (0.75,1.0,1),\n",
    "            (1.0,0.75,1))\n",
    "\n",
    "and_test_data = ((0.1,0.1,0),\n",
    "                 (0.1,0.9,0),\n",
    "                 (0.9,0.1,0),\n",
    "                 (0.9,0.9,1))\n",
    "\n",
    "\"\"\"\n",
    "1-+\n",
    "0+-\n",
    " 01\n",
    "\"\"\"\n",
    "equal_data = ((0,0,1),\n",
    "              (0,1,0),\n",
    "              (1,0,0),\n",
    "              (1,1,1))\n",
    "\n",
    "equal_test_data = ((0.1,0.1,1),\n",
    "                   (0.1,0.9,0),\n",
    "                   (0.9,0.1,0),\n",
    "                   (0.9,0.9,1))\n",
    "\n",
    "\"\"\"\n",
    "1+-\n",
    "0-+\n",
    " 01\n",
    "\"\"\"\n",
    "neq_data = ((0,0,0),\n",
    "            (0,1,1),\n",
    "            (1,0,1),\n",
    "            (1,1,0))\n",
    "\n",
    "neq_test_data = ((0.1,0.1,0),\n",
    "                 (0.1,0.9,1),\n",
    "                 (0.9,0.1,1),\n",
    "                 (0.9,0.9,0))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "3-++-\n",
    "2-++-\n",
    "1-++-\n",
    "0-++-\n",
    " 0123\n",
    "\"\"\"\n",
    "vert_band_data = ((0,0,0),\n",
    "                  (0,1,0),\n",
    "                  (0,2,0),\n",
    "                  (0,3,0),\n",
    "                  (1,0,1),\n",
    "                  (1,1,1),\n",
    "                  (1,2,1),\n",
    "                  (1,3,1),\n",
    "                  (2,0,1),\n",
    "                  (2,1,1),\n",
    "                  (2,2,1),\n",
    "                  (2,3,1),\n",
    "                  (3,0,0),\n",
    "                  (3,1,0),\n",
    "                  (3,2,0),\n",
    "                  (3,3,0))\n",
    "\n",
    "vert_band_test_data = ((0,    1, 0),\n",
    "                        (0,    2, 0),\n",
    "                        (0,  1.5, 0),\n",
    "\n",
    "                        (1.5,  2, 1),\n",
    "                        (1.5,  5, 1),\n",
    "                        (1.5,  1, 1),\n",
    "\n",
    "                        (3,    1, 0),\n",
    "                        (3,  1.5, 0),\n",
    "                        (3,    2, 0),\n",
    "\n",
    "                        (1,  1.5, 1),\n",
    "                        (1, -1.5, 1),\n",
    "                        (2,  1.5, 1),\n",
    "                        (2, -1.5, 1),\n",
    "\n",
    "                        (4,  0,   0),\n",
    "                        (4,  4,   0),\n",
    "                        (-1, 0,   0),\n",
    "                        (-1, 4,   0))\n",
    "\n",
    "\"\"\"\n",
    "3----\n",
    "2++++\n",
    "1++++\n",
    "0----\n",
    " 0123\n",
    "\"\"\"\n",
    "horiz_band_data = ((0,0,0),\n",
    "                   (0,1,1),\n",
    "                   (0,2,1),\n",
    "                   (0,3,0),\n",
    "                   (1,0,0),\n",
    "                   (1,1,1),\n",
    "                   (1,2,1),\n",
    "                   (1,3,0),\n",
    "                   (2,0,0),\n",
    "                   (2,1,1),\n",
    "                   (2,2,1),\n",
    "                   (2,3,0),\n",
    "                   (3,0,0),\n",
    "                   (3,1,1),\n",
    "                   (3,2,1),\n",
    "                   (3,3,0))\n",
    "\n",
    "horiz_band_test_data = ((1, 1.5, 1),\n",
    "                        (2, 1.5, 1),\n",
    "                        (3, 1.5, 1),\n",
    "                        (0, 1.5, 1),\n",
    "                        (4,   0, 0),\n",
    "                        (4,   4, 0),\n",
    "                        (-1,  0, 0),\n",
    "                        (-1,  4, 0))\n",
    "\n",
    "\"\"\"\n",
    "4--- +\n",
    "3-- +\n",
    "2- + -\n",
    "1 + --\n",
    "0+ ---\n",
    " 01234\n",
    "\"\"\"\n",
    "diag_band_data = ((0,0,1),\n",
    "                  (1,1,1),\n",
    "                  (2,2,1),\n",
    "                  (3,3,1),\n",
    "                  (4,4,1),\n",
    "                  (0,4,0),\n",
    "                  (4,0,0),\n",
    "                  (0,3,0),\n",
    "                  (3,0,0),\n",
    "                  (0,2,0),\n",
    "                  (2,0,0),\n",
    "                  (1,4,0),\n",
    "                  (4,1,0),\n",
    "                  (1,3,0),\n",
    "                  (3,1,0),\n",
    "                  (2,4,0),\n",
    "                  (4,2,0),\n",
    "                  )\n",
    "\n",
    "diag_band_test_data = ((-1,-1,1),\n",
    "                       (5,  5,1),\n",
    "                       (-2,-2,1),\n",
    "                       (6,  6,1),\n",
    "                       (3.5,3.5,1),\n",
    "                       (1.5,1.5,1),\n",
    "                       (4,  0,0),\n",
    "                       (0,  4,0))\n",
    "\n",
    "\"\"\"\n",
    "4+++ -\n",
    "3++ -\n",
    "2+ - +\n",
    "1 - ++\n",
    "0- +++\n",
    " 01234\n",
    "\"\"\"\n",
    "idiag_band_data = ((0,0,0),\n",
    "                   (1,1,0),\n",
    "                   (2,2,0),\n",
    "                   (3,3,0),\n",
    "                   (4,4,0),\n",
    "                   (0,4,1),\n",
    "                   (4,0,1),\n",
    "                   (0,3,1),\n",
    "                   (3,0,1),\n",
    "                   (0,2,1),\n",
    "                   (2,0,1),\n",
    "                   (1,4,1),\n",
    "                   (4,1,1),\n",
    "                   (1,3,1),\n",
    "                   (3,1,1),\n",
    "                   (2,4,1),\n",
    "                   (4,2,1),\n",
    "                   )\n",
    "\n",
    "idiag_band_test_data = ((-1,-1,0),\n",
    "                        (5,  5,0),\n",
    "                        (-2,-2,0),\n",
    "                        (6,  6,0),\n",
    "                        (3.5,3.5,0),\n",
    "                        (1.5,1.5,0),\n",
    "                        (4,  0,1),\n",
    "                        (0,  4,1))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "4-----\n",
    "3-   -\n",
    "2- + -\n",
    "1-   -\n",
    "0-----\n",
    " 01234\n",
    "\"\"\"\n",
    "moat_data = ((0,0,0),\n",
    "             (1,0,0),\n",
    "             (2,0,0),\n",
    "             (3,0,0),\n",
    "             (4,0,0),\n",
    "\n",
    "             (1,1,0),\n",
    "             (4,1,0),\n",
    "\n",
    "             (1,2,0),\n",
    "             (3,3,1),\n",
    "             (4,2,0),\n",
    "\n",
    "             (1,4,0),\n",
    "             (4,4,0),\n",
    "\n",
    "             (0,4,0),\n",
    "             (1,4,0),\n",
    "             (2,4,0),\n",
    "             (3,4,0),\n",
    "             (4,4,0),\n",
    "             )\n",
    "\n",
    "moat_test_data = moat_data\n",
    "\n",
    "\"\"\"\n",
    "4+-\n",
    "3+-\n",
    "2+-\n",
    "1+----\n",
    "0-++++\n",
    " 01234\n",
    "\"\"\"\n",
    "letter_l_data = ((0,0,0),\n",
    "                 (1,0,1),\n",
    "                 (2,0,1),\n",
    "                 (3,0,1),\n",
    "                 (4,0,1),\n",
    "\n",
    "                 (1,1,0),\n",
    "                 (2,1,0),\n",
    "                 (3,1,0),\n",
    "                 (4,1,0),\n",
    "\n",
    "                 (0,2,1),\n",
    "                 (1,2,0),\n",
    "\n",
    "                 (0,3,1),\n",
    "                 (1,3,0),\n",
    "\n",
    "                 (0,4,1),\n",
    "                 (1,4,0),\n",
    "                 )\n",
    "\n",
    "letter_l_test_data = letter_l_data\n",
    "\n",
    "\"\"\"\n",
    "4-- ++\n",
    "3-- ++\n",
    "2\n",
    "1++ --\n",
    "0++ --\n",
    " 01234\n",
    "\"\"\"\n",
    "patch_data = ((0,0,1),\n",
    "              (0,1,1),\n",
    "              (1,0,1),\n",
    "              (1,1,1),\n",
    "\n",
    "              (3,0,0),\n",
    "              (3,1,0),\n",
    "              (4,0,0),\n",
    "              (4,1,0),\n",
    "\n",
    "              (0,3,0),\n",
    "              (0,4,0),\n",
    "              (1,3,0),\n",
    "              (1,4,0),\n",
    "\n",
    "              (3,3,1),\n",
    "              (3,4,1),\n",
    "              (4,3,1),\n",
    "              (4,4,1)\n",
    "              )\n",
    "\n",
    "patch_test_data = patch_data\n",
    "\n",
    "simple_data_sets = [(\"OR\", or_data, or_test_data),\n",
    "                    (\"AND\", and_data, and_test_data)\n",
    "                    ]\n",
    "\n",
    "harder_data_sets = [(\"EQUAL\", equal_data, equal_test_data),\n",
    "                    (\"NOT_EQUAL\", neq_data, neq_test_data),\n",
    "                    (\"horizontal-bands\", horiz_band_data, horiz_band_test_data),\n",
    "                    (\"vertical-bands\", vert_band_data, vert_band_test_data),\n",
    "                    (\"diagonal-band\", diag_band_data, diag_band_test_data),\n",
    "                    (\"inverse-diagonal-band\", idiag_band_data,\n",
    "                     idiag_band_test_data)\n",
    "                    ]\n",
    "\n",
    "challenging_data_sets = [(\"moat\", moat_data, moat_test_data),\n",
    "                         (\"letter-l\", letter_l_data, letter_l_test_data),\n",
    "                         ]\n",
    "\n",
    "manual_weight_data_sets = [(\"patchy\", patch_data, patch_test_data)]\n",
    "\n",
    "\n",
    "all_data_sets = simple_data_sets + harder_data_sets + challenging_data_sets + \\\n",
    "                manual_weight_data_sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eH21qW_oaCI"
   },
   "source": [
    "# Neural net tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sBtCpsFyodBk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Training on OR data\n",
      "iter 1000: mean-abs-performance = 0.000212\n",
      "iter 1860: training complete.\n",
      "mean-abs-performance threshold 0.0001 reached (0.000100)\n",
      "Trained weights:\n",
      "Weight 'w1A': 5.112634\n",
      "Weight 'w2A': 5.140964\n",
      "Weight 'wA': 3.133328\n",
      "Weight 'w1B': -2.285830\n",
      "Weight 'w2B': -2.250140\n",
      "Weight 'wB': -1.282177\n",
      "Weight 'AB': 8.708788\n",
      "Weight 'BC': -4.057942\n",
      "Weight 'wC': 2.489752\n",
      "Testing on OR test-data\n",
      "test((0.1, 0.1, 0)) returned: 0.012481795065964512 => 0 [correct]\n",
      "test((0.1, 0.9, 1)) returned: 0.983227445077296 => 1 [correct]\n",
      "test((0.9, 0.1, 1)) returned: 0.983262071109381 => 1 [correct]\n",
      "test((0.9, 0.9, 1)) returned: 0.9974443074680883 => 1 [correct]\n",
      "Accuracy: 1.000000\n",
      "----------------------------------------\n",
      "Training on AND data\n",
      "iter 1000: mean-abs-performance = 0.000500\n",
      "iter 2000: mean-abs-performance = 0.000218\n",
      "iter 3000: mean-abs-performance = 0.000135\n",
      "iter 3894: training complete.\n",
      "mean-abs-performance threshold 0.0001 reached (0.000100)\n",
      "Trained weights:\n",
      "Weight 'w1A': 1.172818\n",
      "Weight 'w2A': -6.804357\n",
      "Weight 'wA': -1.051668\n",
      "Weight 'w1B': -5.838951\n",
      "Weight 'w2B': -2.281970\n",
      "Weight 'wB': -4.485398\n",
      "Weight 'AB': -8.071037\n",
      "Weight 'BC': -9.964722\n",
      "Weight 'wC': -5.029650\n",
      "Testing on AND test-data\n",
      "test((0.1, 0.1, 0)) returned: 6.187166297918634e-05 => 0 [correct]\n",
      "test((0.1, 0.9, 0)) returned: 0.025706537801291604 => 0 [correct]\n",
      "test((0.9, 0.1, 0)) returned: 0.01532322424740231 => 0 [correct]\n",
      "test((0.9, 0.9, 1)) returned: 0.9869793685050254 => 1 [correct]\n",
      "Accuracy: 1.000000\n",
      "----------------------------------------\n",
      "Training on EQUAL data\n",
      "iter 1000: mean-abs-performance = 0.068394\n",
      "iter 2000: mean-abs-performance = 0.067220\n",
      "iter 3000: mean-abs-performance = 0.066965\n",
      "iter 4000: mean-abs-performance = 0.066856\n",
      "iter 5000: mean-abs-performance = 0.066795\n",
      "iter 6000: mean-abs-performance = 0.066757\n",
      "iter 7000: mean-abs-performance = 0.066731\n",
      "iter 8000: mean-abs-performance = 0.066712\n",
      "iter 9000: mean-abs-performance = 0.066697\n",
      "iter 10000: mean-abs-performance = 0.066685\n",
      "Trained weights:\n",
      "Weight 'w1A': -9.790412\n",
      "Weight 'w2A': 3.628401\n",
      "Weight 'wA': 2.596861\n",
      "Weight 'w1B': -9.806760\n",
      "Weight 'w2B': -4.615656\n",
      "Weight 'wB': -0.949765\n",
      "Weight 'AB': -6.389464\n",
      "Weight 'BC': 6.811653\n",
      "Weight 'wC': -0.064396\n",
      "Testing on EQUAL test-data\n",
      "test((0.1, 0.1, 1)) returned: 0.9169035961979274 => 1 [correct]\n",
      "test((0.1, 0.9, 0)) returned: 0.0733445059969709 => 0 [correct]\n",
      "test((0.9, 0.1, 0)) returned: 0.5164749612237488 => 1 [wrong]\n",
      "test((0.9, 0.9, 1)) returned: 0.5156396219154198 => 1 [correct]\n",
      "Accuracy: 0.750000\n",
      "----------------------------------------\n",
      "Training on NOT_EQUAL data\n",
      "iter 1000: mean-abs-performance = 0.093320\n",
      "iter 2000: mean-abs-performance = 0.092169\n",
      "iter 3000: mean-abs-performance = 0.091842\n",
      "iter 4000: mean-abs-performance = 0.091689\n",
      "iter 5000: mean-abs-performance = 0.091600\n",
      "iter 6000: mean-abs-performance = 0.091543\n",
      "iter 7000: mean-abs-performance = 0.091503\n",
      "iter 8000: mean-abs-performance = 0.091473\n",
      "iter 9000: mean-abs-performance = 0.091450\n",
      "iter 10000: mean-abs-performance = 0.091432\n",
      "Trained weights:\n",
      "Weight 'w1A': -7.209786\n",
      "Weight 'w2A': -7.367605\n",
      "Weight 'wA': 0.580296\n",
      "Weight 'w1B': 8.642236\n",
      "Weight 'w2B': 8.783090\n",
      "Weight 'wB': 0.729053\n",
      "Weight 'AB': -3.897446\n",
      "Weight 'BC': 5.020525\n",
      "Weight 'wC': 4.385033\n",
      "Testing on NOT_EQUAL test-data\n",
      "test((0.1, 0.1, 0)) returned: 0.2403578916712239 => 0 [correct]\n",
      "test((0.1, 0.9, 1)) returned: 0.6530505147507568 => 1 [correct]\n",
      "test((0.9, 0.1, 1)) returned: 0.6529641739710398 => 1 [correct]\n",
      "test((0.9, 0.9, 0)) returned: 0.6537323085530309 => 1 [wrong]\n",
      "Accuracy: 0.750000\n",
      "----------------------------------------\n",
      "Training on horizontal-bands data\n",
      "iter 1000: mean-abs-performance = 0.000215\n",
      "iter 1909: training complete.\n",
      "mean-abs-performance threshold 0.0001 reached (0.000100)\n",
      "Trained weights:\n",
      "Weight 'w1A': 0.321409\n",
      "Weight 'w2A': -6.591886\n",
      "Weight 'wA': -2.946659\n",
      "Weight 'w1B': 0.074206\n",
      "Weight 'w2B': -4.191309\n",
      "Weight 'wB': -10.183559\n",
      "Weight 'AB': -10.121307\n",
      "Weight 'BC': 10.417348\n",
      "Weight 'wC': 5.039022\n",
      "Testing on horizontal-bands test-data\n",
      "test((1, 1.5, 1)) returned: 0.9943589447374238 => 1 [correct]\n",
      "test((2, 1.5, 1)) returned: 0.9944061561337224 => 1 [correct]\n",
      "test((3, 1.5, 1)) returned: 0.9944371632666401 => 1 [correct]\n",
      "test((0, 1.5, 1)) returned: 0.9942977301853422 => 1 [correct]\n",
      "test((4, 0, 0)) returned: 0.009967329605668176 => 0 [correct]\n",
      "test((4, 4, 0)) returned: 0.006563559792899223 => 0 [correct]\n",
      "test((-1, 0, 0)) returned: 0.016958133977747106 => 0 [correct]\n",
      "test((-1, 4, 0)) returned: 0.006524544468648731 => 0 [correct]\n",
      "Accuracy: 1.000000\n",
      "----------------------------------------\n",
      "Training on vertical-bands data\n",
      "iter 1000: mean-abs-performance = 0.000207\n",
      "iter 1868: training complete.\n",
      "mean-abs-performance threshold 0.0001 reached (0.000100)\n",
      "Trained weights:\n",
      "Weight 'w1A': 3.981551\n",
      "Weight 'w2A': -0.069072\n",
      "Weight 'wA': 9.846332\n",
      "Weight 'w1B': 6.425880\n",
      "Weight 'w2B': -0.232841\n",
      "Weight 'wB': 3.191879\n",
      "Weight 'AB': -10.924320\n",
      "Weight 'BC': 9.838477\n",
      "Weight 'wC': 4.467923\n",
      "Testing on vertical-bands test-data\n",
      "test((0, 1, 0)) returned: 0.015394392721540367 => 0 [correct]\n",
      "test((0, 2, 0)) returned: 0.014471149307109849 => 0 [correct]\n",
      "test((0, 1.5, 0)) returned: 0.014900428356259838 => 0 [correct]\n",
      "test((1.5, 2, 1)) returned: 0.9942432640666271 => 1 [correct]\n",
      "test((1.5, 5, 1)) returned: 0.9943052264262684 => 1 [correct]\n",
      "test((1.5, 1, 1)) returned: 0.9941945102837393 => 1 [correct]\n",
      "test((3, 1, 0)) returned: 0.013590047970736315 => 0 [correct]\n",
      "test((3, 1.5, 0)) returned: 0.014126409208417994 => 0 [correct]\n",
      "test((3, 2, 0)) returned: 0.014698743689962247 => 0 [correct]\n",
      "test((1, 1.5, 1)) returned: 0.992014354445137 => 1 [correct]\n",
      "test((1, -1.5, 1)) returned: 0.9937587088251747 => 1 [correct]\n",
      "test((2, 1.5, 1)) returned: 0.982914291791272 => 1 [correct]\n",
      "test((2, -1.5, 1)) returned: 0.9779724546164923 => 1 [correct]\n",
      "test((4, 0, 0)) returned: 0.003954953951395574 => 0 [correct]\n",
      "test((4, 4, 0)) returned: 0.003986250726463511 => 0 [correct]\n",
      "test((-1, 0, 0)) returned: 0.011348247031950134 => 0 [correct]\n",
      "test((-1, 4, 0)) returned: 0.011343826882041582 => 0 [correct]\n",
      "Accuracy: 1.000000\n",
      "----------------------------------------\n",
      "Training on diagonal-band data\n",
      "iter 1000: mean-abs-performance = 0.000558\n",
      "iter 2000: mean-abs-performance = 0.000179\n",
      "iter 3000: mean-abs-performance = 0.000106\n",
      "iter 3150: training complete.\n",
      "mean-abs-performance threshold 0.0001 reached (0.000100)\n",
      "Trained weights:\n",
      "Weight 'w1A': 5.032651\n",
      "Weight 'w2A': -4.434141\n",
      "Weight 'wA': -3.638536\n",
      "Weight 'w1B': -4.093533\n",
      "Weight 'w2B': 4.293503\n",
      "Weight 'wB': -4.021319\n",
      "Weight 'AB': 8.672379\n",
      "Weight 'BC': 8.711056\n",
      "Weight 'wC': 13.103383\n",
      "Testing on diagonal-band test-data\n",
      "test((-1, -1, 1)) returned: 0.9758137913083427 => 1 [correct]\n",
      "test((5, 5, 1)) returned: 0.9853930479530211 => 1 [correct]\n",
      "test((-2, -2, 1)) returned: 0.9663923528538708 => 1 [correct]\n",
      "test((6, 6, 1)) returned: 0.985613507901555 => 1 [correct]\n",
      "test((3.5, 3.5, 1)) returned: 0.9848605372649045 => 1 [correct]\n",
      "test((1.5, 1.5, 1)) returned: 0.9832742605064985 => 1 [correct]\n",
      "test((4, 0, 0)) returned: 0.011762974435368438 => 0 [correct]\n",
      "test((0, 4, 0)) returned: 0.012220800179188536 => 0 [correct]\n",
      "Accuracy: 1.000000\n",
      "----------------------------------------\n",
      "Training on inverse-diagonal-band data\n",
      "iter 1000: mean-abs-performance = 0.092157\n",
      "iter 2000: mean-abs-performance = 0.092110\n",
      "iter 3000: mean-abs-performance = 0.092058\n",
      "iter 4000: mean-abs-performance = 0.073897\n",
      "iter 5000: mean-abs-performance = 0.071785\n",
      "iter 6000: mean-abs-performance = 0.071658\n",
      "iter 7000: mean-abs-performance = 0.071612\n",
      "iter 8000: mean-abs-performance = 0.071587\n",
      "iter 9000: mean-abs-performance = 0.071570\n",
      "iter 10000: mean-abs-performance = 0.071559\n",
      "Trained weights:\n",
      "Weight 'w1A': -3.997808\n",
      "Weight 'w2A': -4.037310\n",
      "Weight 'wA': -5.737009\n",
      "Weight 'w1B': 2.427093\n",
      "Weight 'w2B': 2.256687\n",
      "Weight 'wB': 16.836165\n",
      "Weight 'AB': -6.290848\n",
      "Weight 'BC': -5.709863\n",
      "Weight 'wC': -1.871349\n",
      "Testing on inverse-diagonal-band test-data\n",
      "test((-1, -1, 0)) returned: 0.011897106235518281 => 0 [correct]\n",
      "test((5, 5, 0)) returned: 0.021235391854837644 => 0 [correct]\n",
      "test((-2, -2, 0)) returned: 0.011897029059434254 => 0 [correct]\n",
      "test((6, 6, 0)) returned: 0.021073489011863403 => 0 [correct]\n",
      "test((3.5, 3.5, 0)) returned: 0.4106096428684229 => 0 [correct]\n",
      "test((1.5, 1.5, 0)) returned: 0.8652601496579133 => 1 [wrong]\n",
      "test((4, 0, 1)) returned: 0.866058524315373 => 1 [correct]\n",
      "test((0, 4, 1)) returned: 0.8663244219909652 => 1 [correct]\n",
      "Accuracy: 0.875000\n",
      "----------------------------------------\n",
      "Training on moat data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1000: mean-abs-performance = 0.027754\n",
      "iter 2000: mean-abs-performance = 0.027559\n",
      "iter 3000: mean-abs-performance = 0.026735\n",
      "iter 4000: mean-abs-performance = 0.026501\n",
      "iter 5000: mean-abs-performance = 0.026440\n",
      "iter 6000: mean-abs-performance = 0.026417\n",
      "iter 7000: mean-abs-performance = 0.026404\n",
      "iter 8000: mean-abs-performance = 0.026394\n",
      "iter 9000: mean-abs-performance = 0.026185\n",
      "iter 10000: mean-abs-performance = 0.026117\n",
      "Trained weights:\n",
      "Weight 'w1A': 3.270208\n",
      "Weight 'w2A': 0.019565\n",
      "Weight 'wA': 7.043706\n",
      "Weight 'w1B': 1.665486\n",
      "Weight 'w2B': 0.357308\n",
      "Weight 'wB': -1.846129\n",
      "Weight 'wAC': -6.866910\n",
      "Weight 'wBC': 1.521558\n",
      "Weight 'wC': -0.572507\n",
      "Weight 'wABCD': -5.025651\n",
      "Weight 'wD': 1.965215\n",
      "Testing on moat test-data\n",
      "test((0, 0, 0)) returned: 0.0017861629417169195 => 0 [correct]\n",
      "test((1, 0, 0)) returned: 0.001770763097162328 => 0 [correct]\n",
      "test((2, 0, 0)) returned: 0.02062143002929175 => 0 [correct]\n",
      "test((3, 0, 0)) returned: 0.11628377106393863 => 0 [correct]\n",
      "test((4, 0, 0)) returned: 0.11836206343573548 => 0 [correct]\n",
      "test((1, 1, 0)) returned: 0.001760741239536579 => 0 [correct]\n",
      "test((4, 1, 0)) returned: 0.11836305585712394 => 0 [correct]\n",
      "test((1, 2, 0)) returned: 0.0017546112942638714 => 0 [correct]\n",
      "test((3, 3, 1)) returned: 0.11641516223866795 => 0 [wrong]\n",
      "test((4, 2, 0)) returned: 0.11836414373794873 => 0 [correct]\n",
      "test((1, 4, 0)) returned: 0.0017498196685407036 => 0 [correct]\n",
      "test((4, 4, 0)) returned: 0.11836647142644253 => 0 [correct]\n",
      "test((0, 4, 0)) returned: 0.0016446878376216354 => 0 [correct]\n",
      "test((1, 4, 0)) returned: 0.0017498196685407036 => 0 [correct]\n",
      "test((2, 4, 0)) returned: 0.02365709387960809 => 0 [correct]\n",
      "test((3, 4, 0)) returned: 0.11645802204748373 => 0 [correct]\n",
      "test((4, 4, 0)) returned: 0.11836647142644253 => 0 [correct]\n",
      "Accuracy: 0.941176\n",
      "----------------------------------------\n",
      "Training on letter-l data\n",
      "iter 1000: mean-abs-performance = 0.075668\n",
      "iter 2000: mean-abs-performance = 0.075497\n",
      "iter 3000: mean-abs-performance = 0.075454\n",
      "iter 4000: mean-abs-performance = 0.075435\n",
      "iter 5000: mean-abs-performance = 0.075425\n",
      "iter 6000: mean-abs-performance = 0.075418\n",
      "iter 7000: mean-abs-performance = 0.075414\n",
      "iter 8000: mean-abs-performance = 0.075410\n",
      "iter 9000: mean-abs-performance = 0.075408\n",
      "iter 10000: mean-abs-performance = 0.075406\n",
      "Trained weights:\n",
      "Weight 'w1A': 1.721954\n",
      "Weight 'w2A': -7.203935\n",
      "Weight 'wA': 0.948859\n",
      "Weight 'w1B': -2.456748\n",
      "Weight 'w2B': 10.061928\n",
      "Weight 'wB': -1.300649\n",
      "Weight 'wAC': 7.077769\n",
      "Weight 'wBC': -11.321065\n",
      "Weight 'wC': 1.226430\n",
      "Weight 'wABCD': 7.875059\n",
      "Weight 'wD': 0.927625\n",
      "Testing on letter-l test-data\n",
      "test((0, 0, 0)) returned: 0.28386939875196215 => 0 [correct]\n",
      "test((1, 0, 1)) returned: 0.9908155695830375 => 1 [correct]\n",
      "test((2, 0, 1)) returned: 0.9989886803796315 => 1 [correct]\n",
      "test((3, 0, 1)) returned: 0.9990148712643713 => 1 [correct]\n",
      "test((4, 0, 1)) returned: 0.9990174501188975 => 1 [correct]\n",
      "test((1, 1, 0)) returned: 0.2834126558582843 => 0 [correct]\n",
      "test((2, 1, 0)) returned: 0.2834130622060864 => 0 [correct]\n",
      "test((3, 1, 0)) returned: 0.28341670075910896 => 0 [correct]\n",
      "test((4, 1, 0)) returned: 0.28360753154371887 => 0 [correct]\n",
      "test((0, 2, 1)) returned: 0.28341258195982655 => 0 [wrong]\n",
      "test((1, 2, 0)) returned: 0.28341258199975056 => 0 [correct]\n",
      "test((0, 3, 1)) returned: 0.28341258195118746 => 0 [wrong]\n",
      "test((1, 3, 0)) returned: 0.2834125819512169 => 0 [correct]\n",
      "test((0, 4, 1)) returned: 0.283412581951181 => 0 [wrong]\n",
      "test((1, 4, 0)) returned: 0.2834125819511811 => 0 [correct]\n",
      "Accuracy: 0.800000\n",
      "----------------------------------------\n",
      "Training on patchy data\n",
      "iter 1000: mean-abs-performance = 0.087036\n",
      "iter 2000: mean-abs-performance = 0.086965\n",
      "iter 3000: mean-abs-performance = 0.086944\n",
      "iter 4000: mean-abs-performance = 0.086935\n",
      "iter 5000: mean-abs-performance = 0.086930\n",
      "iter 6000: mean-abs-performance = 0.086927\n",
      "iter 7000: mean-abs-performance = 0.086924\n",
      "iter 8000: mean-abs-performance = 0.086922\n",
      "iter 9000: mean-abs-performance = 0.086921\n",
      "iter 10000: mean-abs-performance = 0.086920\n",
      "Trained weights:\n",
      "Weight 'w1A': -4.177480\n",
      "Weight 'w2A': 4.192581\n",
      "Weight 'wA': -5.971722\n",
      "Weight 'w1B': -2.420461\n",
      "Weight 'w2B': 2.753361\n",
      "Weight 'wB': -2.757439\n",
      "Weight 'wAC': -9.901147\n",
      "Weight 'wBC': -4.898422\n",
      "Weight 'wC': -2.536305\n",
      "Weight 'wABCD': -7.973946\n",
      "Weight 'wD': -0.761610\n",
      "Testing on patchy test-data\n",
      "test((0, 0, 1)) returned: 0.6816920400139389 => 1 [correct]\n",
      "test((0, 1, 1)) returned: 0.6816949267765346 => 1 [correct]\n",
      "test((1, 0, 1)) returned: 0.6814454046508315 => 1 [correct]\n",
      "test((1, 1, 1)) returned: 0.6816928980740631 => 1 [correct]\n",
      "test((3, 0, 0)) returned: 0.001372538436080356 => 0 [correct]\n",
      "test((3, 1, 0)) returned: 0.006625112644501142 => 0 [correct]\n",
      "test((4, 0, 0)) returned: 0.0013254187290700657 => 0 [correct]\n",
      "test((4, 1, 0)) returned: 0.0013893629283603728 => 0 [correct]\n",
      "test((0, 3, 0)) returned: 0.6816950920467132 => 1 [wrong]\n",
      "test((0, 4, 0)) returned: 0.6816950926623939 => 1 [wrong]\n",
      "test((1, 3, 0)) returned: 0.6816950852660334 => 1 [wrong]\n",
      "test((1, 4, 0)) returned: 0.6816950922327132 => 1 [wrong]\n",
      "test((3, 3, 1)) returned: 0.6816939029231498 => 1 [correct]\n",
      "test((3, 4, 1)) returned: 0.6816950299746364 => 1 [correct]\n",
      "test((4, 3, 1)) returned: 0.6816152070353763 => 1 [correct]\n",
      "test((4, 4, 1)) returned: 0.6816941933270266 => 1 [correct]\n",
      "Accuracy: 0.750000\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Unit tester for Neural net code.\n",
    "#\n",
    "import sys\n",
    "\n",
    "def main(neural_net_func, data_sets, max_iterations=10000):\n",
    "    verbose = True\n",
    "    for name, training_data, test_data in data_sets:\n",
    "        print(\"-\"*40)\n",
    "        print(\"Training on %s data\" %(name))\n",
    "        nn = neural_net_func()\n",
    "        train(nn, training_data, max_iterations=max_iterations,\n",
    "              verbose=verbose)\n",
    "        print(\"Trained weights:\")\n",
    "        for w in nn.weights:\n",
    "            print(\"Weight '%s': %f\"%(w.get_name(),w.get_value()))\n",
    "        print(\"Testing on %s test-data\" %(name))\n",
    "        result = test(nn, test_data, verbose=verbose)\n",
    "        print(\"Accuracy: %f\"%(result))\n",
    "\n",
    "def neural_net_tester(arg=None):\n",
    "    test_names = [\"simple\"]\n",
    "    if arg is not None:\n",
    "        test_names = arg\n",
    "\n",
    "    for test_name in test_names:\n",
    "        if test_name == \"simple\":\n",
    "            # these test simple logical configurations\n",
    "            main(make_neural_net_basic,\n",
    "                 simple_data_sets)\n",
    "\n",
    "        elif test_name == \"two_layer\":\n",
    "            # these test cases are slightly harder\n",
    "            main(make_neural_net_two_layer,\n",
    "                 simple_data_sets + harder_data_sets)\n",
    "\n",
    "        elif test_name == \"challenging\":\n",
    "            # these tests require a more complex architecture.\n",
    "            main(make_neural_net_challenging, challenging_data_sets)\n",
    "\n",
    "        elif test_name == \"patchy\":\n",
    "            # patchy problem is slightly tricky\n",
    "            # unless your network gets the right weights.\n",
    "            # it can quickly get stuck in local maxima.\n",
    "            main(make_neural_net_challenging, manual_weight_data_sets)\n",
    "        else:\n",
    "            print(\"unrecognized test name %s\" %(test_name))\n",
    "            \n",
    "#neural_net_tester([\"simple\"])\n",
    "neural_net_tester([\"two_layer\"])\n",
    "neural_net_tester([\"challenging\"])\n",
    "neural_net_tester([\"patchy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REsh3Hp0zW_Y"
   },
   "source": [
    "### Net Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "egoThwghzYvh"
   },
   "outputs": [],
   "source": [
    "def neural_net_tester(network_maker_func,\n",
    "                      train_dataset_name,\n",
    "                      test_dataset_name,\n",
    "                      iterations):\n",
    "    \"\"\"Test a neural net making function on a named dataset\"\"\"\n",
    "    seed_random()\n",
    "    network_maker_func = globals()[network_maker_func]\n",
    "    train_dataset = globals()[train_dataset_name]\n",
    "    test_dataset = globals()[test_dataset_name]\n",
    "    nn = network_maker_func()\n",
    "\n",
    "    train(nn, train_dataset, max_iterations=iterations)\n",
    "    result = test(nn, test_dataset)\n",
    "    return result\n",
    "\n",
    "def neural_net_size_tester(network_maker_func):\n",
    "    \"\"\"Test a neural net size\"\"\"\n",
    "    network_maker_func = globals()[network_maker_func]\n",
    "    nn = network_maker_func()\n",
    "    return len(nn.neurons)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr4IqJ8_0m-U"
   },
   "source": [
    "# Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PqQoKaNT0srX"
   },
   "outputs": [],
   "source": [
    "from xmlrpc import client\n",
    "import traceback\n",
    "import sys\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "try:\n",
    "    from StringIO import StringIO\n",
    "except ImportError:\n",
    "    from io import StringIO\n",
    "\n",
    "\n",
    "# This is a skeleton for what the tester should do. Ideally, this module\n",
    "# would be imported in the pset and run as its main function.\n",
    "\n",
    "# We need the following rpc functions. (They generally take username and\n",
    "# password, but you could adjust this for whatever security system.)\n",
    "#\n",
    "# tester.submit_code(username, password, pset, studentcode)\n",
    "#   'pset' is a string such as 'ps0'. studentcode is a string containing\n",
    "#   the contents of the corresponding file, ps0.py. This stores the code on\n",
    "#   the server so we can check it later for cheating, and is a prerequisite\n",
    "#   to the tester returning a grade.\n",
    "#\n",
    "# tester.get_tests(pset)\n",
    "#   returns a list of tuples of the form (INDEX, TYPE, NAME, ARGS):\n",
    "#     INDEX is a unique integer that identifies the test.\n",
    "#     TYPE should be one of either 'VALUE' or 'FUNCTION'.\n",
    "#     If TYPE is 'VALUE', ARGS is ignored, and NAME is the name of a\n",
    "#     variable to return for this test.  The variable must be an attribute\n",
    "#     of the lab module.\n",
    "#     If TYPE is 'FUNCTION', NAME is the name of a function in the lab module\n",
    "#     whose return value should be the answer to this test, and ARGS is a\n",
    "#     tuple containing arguments for the function.\n",
    "#\n",
    "# tester.send_answer(username, password, pset, index, answer)\n",
    "#   Sends <answer> as the answer to test case <index> (0-numbered) in the pset\n",
    "#   named <pset>. Returns whether the answer was correct, and an expected\n",
    "#   value.\n",
    "#\n",
    "# tester.status(username, password, pset)\n",
    "#   A string that includes the official score for this user on this pset.\n",
    "#   If a part is missing (like the code), it should say so.\n",
    "\n",
    "# Because I haven't written anything on the server side, test_online has never\n",
    "# been tested.\n",
    "\n",
    "def test_summary(dispindex, ntests, testname):\n",
    "    return \"Test %d/%d (%s)\" % (dispindex, ntests, testname)\n",
    "\n",
    "tests = []\n",
    "\n",
    "def show_result(testsummary, testcode, correct, got, expected, verbosity):\n",
    "    \"\"\" Pretty-print test results \"\"\"\n",
    "    if correct:\n",
    "        if verbosity > 0:\n",
    "            print(\"%s: Correct.\" % testsummary)\n",
    "        if verbosity > 1:\n",
    "            print('\\t', testcode)\n",
    "            print('')\n",
    "    else:\n",
    "        print(\"%s: Incorrect.\" % testsummary)\n",
    "        print('\\t', testcode)\n",
    "        print(\"Got:     \", got)\n",
    "        print(\"Expected:\", expected)\n",
    "\n",
    "def show_exception(testsummary, testcode):\n",
    "    \"\"\" Pretty-print exceptions (including tracebacks) \"\"\"\n",
    "    print(\"%s: Error.\" % testsummary)\n",
    "    print(\"While running the following test case:\")\n",
    "    print('\\t', testcode)\n",
    "    print(\"Your code encountered the following error:\")\n",
    "    traceback.print_exc()\n",
    "    print('')\n",
    "\n",
    "\n",
    "def get_lab_module():\n",
    "    # Try the easy way first\n",
    "    try:\n",
    "        from tests import lab_number\n",
    "    except ImportError:\n",
    "        lab_number = None\n",
    "\n",
    "    if lab_number != None:\n",
    "        lab = __import__('lab%s' % lab_number)\n",
    "        return lab\n",
    "\n",
    "    lab = None\n",
    "\n",
    "    for labnum in range(10):\n",
    "        try:\n",
    "            lab = __import__('lab%s' % labnum)\n",
    "        except ImportError:\n",
    "            pass\n",
    "\n",
    "    if lab == None:\n",
    "        raise ImportError(\"Cannot find your lab; or, error importing it.  Try loading it by running 'python labN.py' (for the appropriate value of 'N').\")\n",
    "\n",
    "    if not hasattr(lab, \"LAB_NUMBER\"):\n",
    "        lab.LAB_NUMBER = labnum\n",
    "\n",
    "    return lab\n",
    "\n",
    "def type_decode(arg, lab):\n",
    "    \"\"\"\n",
    "    XMLRPC can only pass a very limited collection of types.\n",
    "    Frequently, we want to pass a subclass of 'list' in as a test argument.\n",
    "    We do that by converting the sub-type into a regular list of the form:\n",
    "    [ 'TYPE', (data) ] (ie., AND(['x','y','z']) becomes ['AND','x','y','z']).\n",
    "    This function assumes that TYPE is a valid attr of 'lab' and that TYPE's\n",
    "    constructor takes a list as an argument; it uses that to reconstruct the\n",
    "    original data type.\n",
    "    \"\"\"\n",
    "    if isinstance(arg, list) and len(arg) >= 1: # We'll leave tuples reserved for some other future magic\n",
    "        try:\n",
    "            mytype = arg[0]\n",
    "            data = arg[1:]\n",
    "            return getattr(lab, mytype)([ type_decode(x, lab) for x in data ])\n",
    "        except AttributeError:\n",
    "            return [ type_decode(x, lab) for x in arg ]\n",
    "        except TypeError:\n",
    "            return [ type_decode(x, lab) for x in arg ]\n",
    "    else:\n",
    "        return arg\n",
    "\n",
    "\n",
    "def type_encode(arg):\n",
    "    \"\"\"\n",
    "    Encode trees as lists in a way that can be decoded by 'type_decode'\n",
    "    \"\"\"\n",
    "    if isinstance(arg, list) and not type(arg) in (list,tuple):\n",
    "        return [ arg.__class__.__name__ ] + [ type_encode(x) for x in arg ]\n",
    "    elif hasattr(arg, '__class__') and arg.__class__.__name__ == 'IF':\n",
    "        return [ 'IF', type_encode(arg._conditional), type_encode(arg._action), type_encode(arg._delete_clause) ]\n",
    "    else:\n",
    "        return arg\n",
    "\n",
    "\n",
    "def run_test(test, lab):\n",
    "    \"\"\"\n",
    "    Takes a 'test' tuple as provided by the online tester\n",
    "    (or generated by the offline tester) and executes that test,\n",
    "    returning whatever output is expected (the variable that's being\n",
    "    queried, the output of the function being called, etc)\n",
    "\n",
    "    'lab' (the argument) is the module containing the lab code.\n",
    "\n",
    "    'test' tuples are in the following format:\n",
    "      'id': A unique integer identifying the test\n",
    "      'type': One of 'VALUE', 'FUNCTION', 'MULTIFUNCTION', or 'FUNCTION_ENCODED_ARGS'\n",
    "      'attr_name': The name of the attribute in the 'lab' module\n",
    "      'args': a list of the arguments to be passed to the function; [] if no args.\n",
    "      For 'MULTIFUNCTION's, a list of lists of arguments to be passed in\n",
    "    \"\"\"\n",
    "    id, mytype, attr_name, args = test\n",
    "\n",
    "    attr = getattr(lab, attr_name)\n",
    "\n",
    "    if mytype == 'VALUE':\n",
    "        return attr\n",
    "    elif mytype == 'FUNCTION':\n",
    "        return apply(attr, args)\n",
    "    elif mytype == 'MULTIFUNCTION':\n",
    "        return [ run_test( (id, 'FUNCTION', attr_name, FN), lab) for FN in args ]\n",
    "    elif mytype == 'FUNCTION_ENCODED_ARGS':\n",
    "        return run_test( (id, 'FUNCTION', attr_name, type_decode(args, lab)), lab )\n",
    "    else:\n",
    "        raise Exception(\"Test Error: Unknown TYPE '%s'.  Please make sure you have downloaded the latest version of the tester script.  If you continue to see this error, contact a TA.\")\n",
    "\n",
    "\n",
    "def test_offline(verbosity=1):\n",
    "    \"\"\" Run the unit tests in 'tests.py' \"\"\"\n",
    "    import tests as tests_module\n",
    "\n",
    "#    tests = [ (x[:-8],\n",
    "#               getattr(tests_module, x),\n",
    "#               getattr(tests_module, \"%s_testanswer\" % x[:-8]),\n",
    "#               getattr(tests_module, \"%s_expected\" % x[:-8]),\n",
    "#               \"_\".join(x[:-8].split('_')[:-1]))\n",
    "#              for x in tests_module.__dict__.keys() if x[-8:] == \"_getargs\" ]\n",
    "\n",
    "#    tests = tests_module.get_tests()\n",
    "    global tests \n",
    "\n",
    "    ntests = len(tests)\n",
    "    ncorrect = 0\n",
    "\n",
    "    for index, (testname, getargs, testanswer, expected, fn_name, type) in enumerate(tests):\n",
    "        dispindex = index+1\n",
    "        summary = test_summary(dispindex, ntests, fn_name)\n",
    "\n",
    "        try:\n",
    "            if callable(getargs):\n",
    "                getargs = getargs()\n",
    "\n",
    "            if type == 'FUNCTION':\n",
    "                answer = fn_name(*getargs)\n",
    "            elif type == 'VALUE':\n",
    "                answer = fn_name\n",
    "            else:\n",
    "                answer = [ FN(*getargs) for FN in getargs ]#run_test((index, type, fn_name, getargs), get_lab_module())\n",
    "        except NotImplementedError:\n",
    "            print(\"%d: (%s: Function not yet implemented, NotImplementedError raised)\" % (index, testname))\n",
    "            continue\n",
    "        except Exception:\n",
    "            show_exception(summary, testname)\n",
    "            continue\n",
    "\n",
    "        correct = testanswer(answer, original_val = getargs)\n",
    "        show_result(summary, testname, correct, answer, expected, verbosity)\n",
    "        if correct: ncorrect += 1\n",
    "\n",
    "    print(\"Passed %d of %d tests.\" % (ncorrect, ntests))\n",
    "    tests = []\n",
    "    return (ncorrect == ntests)\n",
    "\n",
    "\n",
    "\n",
    "def get_target_upload_filedir():\n",
    "    \"\"\" Get, via user prompting, the directory containing the current lab \"\"\"\n",
    "    cwd = os.getcwd() # Get current directory.  Play nice with Unicode pathnames, just in case.\n",
    "\n",
    "    print(\"Please specify the directory containing your lab.\")\n",
    "    print(\"Note that all files from this directory will be uploaded!\")\n",
    "    print(\"Labs should not contain large amounts of data; very-large\")\n",
    "    print(\"files will fail to upload.\")\n",
    "    print('')\n",
    "    print(\"The default path is '%s'\" % cwd)\n",
    "    target_dir = raw_input(\"[%s] >>> \" % cwd)\n",
    "\n",
    "    target_dir = target_dir.strip()\n",
    "    if target_dir == '':\n",
    "        target_dir = cwd\n",
    "\n",
    "    print(\"Ok, using '%s'.\" % target_dir)\n",
    "\n",
    "    return target_dir\n",
    "\n",
    "def get_tarball_data(target_dir, filename):\n",
    "    \"\"\" Return a binary String containing the binary data for a tarball of the specified directory \"\"\"\n",
    "    data = StringIO()\n",
    "    file = tarfile.open(filename, \"w|bz2\", data)\n",
    "\n",
    "    print(\"Preparing the lab directory for transmission...\")\n",
    "\n",
    "    file.add(target_dir+\"/lab5.py\")\n",
    "    file.add(target_dir+\"/neural_net.py\")\n",
    "    file.add(target_dir+\"/boost.py\")\n",
    "    file.add(target_dir+\"/key.py\")\n",
    "\n",
    "    print(\"Done.\")\n",
    "    print('')\n",
    "    print(\"The following files have been added:\")\n",
    "\n",
    "    for f in file.getmembers():\n",
    "        print(f.name)\n",
    "\n",
    "    file.close()\n",
    "\n",
    "    return data.getvalue()\n",
    "\n",
    "\n",
    "def test_online(verbosity=1):\n",
    "    \"\"\" Run online unit tests.  Run them against the 6.034 server via XMLRPC. \"\"\"\n",
    "    lab = get_lab_module()\n",
    "\n",
    "    try:\n",
    "        server = xmlrpclib.Server(server_url, allow_none=True)\n",
    "        print(\"Getting tests:\", (username, password, lab.__name__))\n",
    "        tests = server.get_tests(username, password, lab.__name__)\n",
    "        print(\"*** TESTS:\")\n",
    "        print(tests)\n",
    "\n",
    "    except NotImplementedError: # Solaris Athena doesn't seem to support HTTPS\n",
    "        print(\"Your version of Python doesn't seem to support HTTPS, for\")\n",
    "        print(\"secure test submission.  Would you like to downgrade to HTTP?\")\n",
    "        print(\"(note that this could theoretically allow a hacker with access\")\n",
    "        print(\"to your local network to find your 6.034 password)\")\n",
    "        answer = raw_input(\"(Y/n) >>> \")\n",
    "        if len(answer) == 0 or answer[0] in \"Yy\":\n",
    "            server = xmlrpclib.Server(server_url.replace(\"https\", \"http\"))\n",
    "            tests = server.get_tests(username, password, lab.__name__)\n",
    "        else:\n",
    "            print(\"Ok, not running your tests.\")\n",
    "            print(\"Please try again on another computer.\")\n",
    "            print(\"Linux Athena computers are known to support HTTPS,\")\n",
    "            print(\"if you use the version of Python in the 'python' locker.\")\n",
    "            sys.exit(0)\n",
    "\n",
    "    ntests = len(tests)\n",
    "    ncorrect = 0\n",
    "\n",
    "    lab = get_lab_module()\n",
    "\n",
    "    target_dir = get_target_upload_filedir()\n",
    "\n",
    "    tarball_data = get_tarball_data(target_dir, \"lab%s.tar.bz2\" % lab.LAB_NUMBER)\n",
    "\n",
    "    print(\"Submitting to the 6.034 Webserver...\")\n",
    "\n",
    "    server.submit_code(username, password, lab.__name__, xmlrpclib.Binary(tarball_data))\n",
    "\n",
    "    print(\"Done submitting code.\")\n",
    "    print(\"Running test cases...\")\n",
    "\n",
    "    for index, testcode in enumerate(tests):\n",
    "        dispindex = index+1\n",
    "        summary = test_summary(dispindex, ntests, testcode)\n",
    "\n",
    "        try:\n",
    "            answer = run_test(testcode, get_lab_module())\n",
    "        except Exception:\n",
    "            show_exception(summary, testcode)\n",
    "            continue\n",
    "\n",
    "        correct, expected = server.send_answer(username, password, lab.__name__, testcode[0], type_encode(answer))\n",
    "        show_result(summary, testcode, correct, answer, expected, verbosity)\n",
    "        if correct: ncorrect += 1\n",
    "\n",
    "    response = server.status(username, password, lab.__name__)\n",
    "    print(response)\n",
    "\n",
    "\n",
    "def make_test_counter_decorator():\n",
    "    #tests = []\n",
    "    def make_test(getargs, testanswer, expected_val, name = None, type = 'FUNCTION'):\n",
    "        if name != None:\n",
    "            getargs_name = name\n",
    "        elif not callable(getargs):\n",
    "            getargs_name = \"_\".join(getargs[:-8].split('_')[:-1])\n",
    "            getargs = lambda: getargs\n",
    "        else:\n",
    "            getargs_name = \"_\".join(getargs.__name__[:-8].split('_')[:-1])\n",
    "\n",
    "        tests.append( ( getargs_name,\n",
    "                        getargs,\n",
    "                        testanswer,\n",
    "                        expected_val,\n",
    "                        getargs_name,\n",
    "                        type ) )\n",
    "\n",
    "    def get_tests():\n",
    "        return tests\n",
    "\n",
    "    return make_test, get_tests\n",
    "\n",
    "\n",
    "make_test, get_tests = make_test_counter_decorator()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "na4Dh04k0w8H"
   },
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hUwtBcjB0ykF",
    "outputId": "e5669970-60a9-4656-c98c-3373213548de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1/8 (<function neural_net_size_tester at 0x000002041BA8ACA0>): Correct.\n",
      "Test 2/8 (<function neural_net_tester at 0x000002041BA8AC00>): Correct.\n",
      "Test 3/8 (<function neural_net_tester at 0x000002041BA8AC00>): Correct.\n",
      "Test 4/8 (<function neural_net_tester at 0x000002041BA8AC00>): Correct.\n",
      "Test 5/8 (<function neural_net_tester at 0x000002041BA8AC00>): Correct.\n",
      "Test 6/8 (<function neural_net_tester at 0x000002041BA8AC00>): Correct.\n",
      "Test 7/8 (<function neural_net_size_tester at 0x000002041BA8ACA0>): Correct.\n",
      "Test 8/8 (<function neural_net_tester at 0x000002041BA8AC00>): Correct.\n",
      "Passed 8 of 8 tests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = 'your trained neural-net on %s data must test with an accuracy of %1.3f'\n",
    "expected_accuracy = 1.0\n",
    "\n",
    "def neural_net_test_testanswer(val, original_val = None):\n",
    "    return abs(val - expected_accuracy) < 0.01\n",
    "\n",
    "network_maker_func = \"make_neural_net_two_layer\"\n",
    "network_min_size = 3\n",
    "max_iterations = 10000\n",
    "\n",
    "def neural_net_size_testanswer(val, original_val = None):\n",
    "    return val <= network_min_size\n",
    "\n",
    "make_test(type = 'FUNCTION',\n",
    "          getargs = lambda: [network_maker_func],\n",
    "          testanswer = neural_net_size_testanswer,\n",
    "          expected_val = \"your network must have <= %d neural units\"\\\n",
    "          %(network_min_size),\n",
    "          name = neural_net_size_tester\n",
    "          )\n",
    "\n",
    "make_test(type = 'FUNCTION',\n",
    "          getargs = lambda: [network_maker_func,\n",
    "                             'and_data',\n",
    "                             'and_test_data',\n",
    "                             max_iterations],\n",
    "          testanswer = neural_net_test_testanswer,\n",
    "          expected_val = message %(\"AND\", expected_accuracy),\n",
    "          name = neural_net_tester\n",
    "          )\n",
    "\n",
    "make_test(type = 'FUNCTION',\n",
    "          getargs = lambda: [network_maker_func,\n",
    "                             'or_data',\n",
    "                             'or_test_data',\n",
    "                             max_iterations],\n",
    "          testanswer = neural_net_test_testanswer,\n",
    "          expected_val = message %(\"OR\", expected_accuracy),\n",
    "          name = neural_net_tester\n",
    "          )\n",
    "\n",
    "make_test(type = 'FUNCTION',\n",
    "          getargs = lambda: [network_maker_func,\n",
    "                             'neq_data',\n",
    "                             'neq_test_data',\n",
    "                             max_iterations],\n",
    "          testanswer = neural_net_test_testanswer,\n",
    "          expected_val = message %(\"XOR\", expected_accuracy),\n",
    "          name = neural_net_tester\n",
    "          )\n",
    "\n",
    "make_test(type = 'FUNCTION',\n",
    "          getargs = lambda: [network_maker_func,\n",
    "                             'equal_data',\n",
    "                             'equal_test_data',\n",
    "                             max_iterations],\n",
    "          testanswer = neural_net_test_testanswer,\n",
    "          expected_val = message %(\"EQUAL\", expected_accuracy),\n",
    "          name = neural_net_tester\n",
    "          )\n",
    "\n",
    "make_test(type = 'FUNCTION',\n",
    "          getargs = lambda: [network_maker_func,\n",
    "                             'diag_band_data',\n",
    "                             'diag_band_test_data',\n",
    "                             max_iterations],\n",
    "          testanswer = neural_net_test_testanswer,\n",
    "          expected_val = message %(\"diagonal_band\", expected_accuracy),\n",
    "          name = neural_net_tester\n",
    "          )\n",
    "\n",
    "challenging_network_maker_func = \"make_neural_net_challenging\"\n",
    "challenging_network_min_size = 5\n",
    "\n",
    "def challenging_neural_net_size_testanswer(val, original_val = None):\n",
    "    return val <= challenging_network_min_size\n",
    "\n",
    "make_test(type = 'FUNCTION',\n",
    "          getargs = lambda: [challenging_network_maker_func],\n",
    "          testanswer = challenging_neural_net_size_testanswer,\n",
    "          expected_val = \"your network must have <= %d neural units\"\\\n",
    "          %(network_min_size),\n",
    "          name = neural_net_size_tester\n",
    "          )\n",
    "\n",
    "make_test(type = 'FUNCTION',\n",
    "          getargs = lambda: [challenging_network_maker_func,\n",
    "                             'letter_l_data',\n",
    "                             'letter_l_test_data',\n",
    "                             max_iterations],\n",
    "          testanswer = neural_net_test_testanswer,\n",
    "          expected_val = message %(\"letter-l\", expected_accuracy),\n",
    "          name = neural_net_tester\n",
    "          )\n",
    "\n",
    "test_offline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "GVB7klrtGHHX",
    "569hIE5WoDHf",
    "1eH21qW_oaCI",
    "Pr9fMhYNy4e3",
    "REsh3Hp0zW_Y",
    "dr4IqJ8_0m-U",
    "na4Dh04k0w8H"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
